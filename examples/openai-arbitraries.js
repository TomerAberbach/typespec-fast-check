// Generated from TypeSpec using `typespec-fast-check`

import * as fc from "fast-check";

const ImagesN = fc.integer({
  min: 1,
  max: 10,
});

const User = fc.string();

/** Represents the url or the content of an image generated by the OpenAI API. */
const Image = fc.record(
  {
    /** The URL of the generated image, if `response_format` is `url` (default). */
    url: fc.webUrl(),
    /** The base64-encoded JSON of the generated image, if `response_format` is `b64_json`. */
    b64_json: fc.uint8Array(),
  },
  { withDeletedKeys: true },
);

const FineTuneEvent = fc.record({
  object: fc.string(),
  created_at: fc.string(),
  level: fc.string(),
  message: fc.string(),
});

/** The `File` object represents a document that has been uploaded to OpenAI. */
const OpenAIFile = fc.record(
  {
    /** The file identifier, which can be referenced in the API endpoints. */
    id: fc.string(),
    /** The object type, which is always "file". */
    object: fc.constant("file"),
    /** The size of the file in bytes. */
    bytes: fc.maxSafeInteger(),
    /** The Unix timestamp (in seconds) for when the file was created. */
    createdAt: fc.string(),
    /** The name of the file. */
    filename: fc.string(),
    /** The intended purpose of the file. Currently, only "fine-tune" is supported. */
    purpose: fc.string(),
    /**
     * The current status of the file, which can be either `uploaded`, `processed`, `pending`,
     * `error`, `deleting` or `deleted`.
     */
    status: fc.constantFrom("uploaded", "processed", "pending", "error", "deleting", "deleted"),
    /**
     * Additional details about the status of the file. If the file is in the `error` state, this will
     * include a message describing the error.
     */
    status_details: fc.option(fc.string()),
  },
  {
    requiredKeys: ["id", "object", "bytes", "createdAt", "filename", "purpose", "status"],
  },
);

/** The `FineTune` object represents a legacy fine-tune job that has been created through the API. */
const FineTune = fc.record(
  {
    /** The object identifier, which can be referenced in the API endpoints. */
    id: fc.string(),
    /** The object type, which is always "fine-tune". */
    object: fc.constant("fine-tune"),
    /** The Unix timestamp (in seconds) for when the fine-tuning job was created. */
    created_at: fc.string(),
    /** The Unix timestamp (in seconds) for when the fine-tuning job was last updated. */
    updated_at: fc.string(),
    /** The base model that is being fine-tuned. */
    model: fc.string(),
    /** The name of the fine-tuned model that is being created. */
    fine_tuned_model: fc.option(fc.string()),
    /** The organization that owns the fine-tuning job. */
    organization_id: fc.string(),
    /**
     * The current status of the fine-tuning job, which can be either `created`, `running`,
     * `succeeded`, `failed`, or `cancelled`.
     */
    status: fc.constantFrom("created", "running", "succeeded", "failed", "cancelled"),
    /**
     * The hyperparameters used for the fine-tuning job. See the
     * [fine-tuning guide](/docs/guides/legacy-fine-tuning/hyperparameters) for more details.
     */
    hyperparams: fc.record(
      {
        /**
         * The number of epochs to train the model for. An epoch refers to one full cycle through the
         * training dataset.
         */
        n_epochs: fc.maxSafeInteger(),
        /**
         * The batch size to use for training. The batch size is the number of training examples used to
         * train a single forward and backward pass.
         */
        batch_size: fc.maxSafeInteger(),
        /** The weight to use for loss on the prompt tokens. */
        prompt_loss_weight: fc.double(),
        /** The learning rate multiplier to use for training. */
        learning_rate_multiplier: fc.double(),
        /** The classification metrics to compute using the validation dataset at the end of every epoch. */
        compute_classification_metrics: fc.boolean(),
        /** The positive class to use for computing classification metrics. */
        classification_positive_class: fc.string(),
        /** The number of classes to use for computing classification metrics. */
        classification_n_classes: fc.maxSafeInteger(),
      },
      {
        requiredKeys: ["n_epochs", "batch_size", "prompt_loss_weight", "learning_rate_multiplier"],
      },
    ),
    /** The list of files used for training. */
    training_files: fc.array(OpenAIFile),
    /** The list of files used for validation. */
    validation_files: fc.array(OpenAIFile),
    /** The compiled results files for the fine-tuning job. */
    result_files: fc.array(OpenAIFile),
    /** The list of events that have been observed in the lifecycle of the FineTune job. */
    events: fc.array(FineTuneEvent),
  },
  {
    requiredKeys: ["id", "object", "created_at", "updated_at", "model", "fine_tuned_model", "organization_id", "status", "hyperparams", "training_files", "validation_files", "result_files"],
  },
);

const SuffixString = fc.string({
  minLength: 1,
  maxLength: 40,
});

const FineTuningJobEvent = fc.record({
  id: fc.string(),
  object: fc.string(),
  created_at: fc.string(),
  level: fc.constantFrom("info", "warn", "error"),
  message: fc.string(),
});

const NEpochs = fc.integer({
  min: 1,
  max: 50,
});

const FineTuningJob = fc.record({
  /** The object identifier, which can be referenced in the API endpoints. */
  id: fc.string(),
  /** The object type, which is always "fine_tuning.job". */
  object: fc.constant("fine_tuning.job"),
  /** The Unix timestamp (in seconds) for when the fine-tuning job was created. */
  created_at: fc.string(),
  /**
   * The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be
   * null if the fine-tuning job is still running.
   */
  finished_at: fc.option(fc.string()),
  /** The base model that is being fine-tuned. */
  model: fc.string(),
  /**
   * The name of the fine-tuned model that is being created. The value will be null if the
   * fine-tuning job is still running.
   */
  fine_tuned_model: fc.option(fc.string()),
  /** The organization that owns the fine-tuning job. */
  organization_id: fc.string(),
  /**
   * The current status of the fine-tuning job, which can be either `created`, `pending`, `running`,
   * `succeeded`, `failed`, or `cancelled`.
   */
  status: fc.constantFrom("created", "pending", "running", "succeeded", "failed", "cancelled"),
  /**
   * The hyperparameters used for the fine-tuning job. See the
   * [fine-tuning guide](/docs/guides/fine-tuning) for more details.
   */
  hyperparameters: fc.record(
    {
      /**
       * The number of epochs to train the model for. An epoch refers to one full cycle through the
       * training dataset.
       *
       * "Auto" decides the optimal number of epochs based on the size of the dataset. If setting the
       * number manually, we support any number between 1 and 50 epochs.
       */
      n_epochs: fc.oneof(
        NEpochs,
        fc.constant("auto"),
      ),
    },
    { withDeletedKeys: true },
  ),
  /**
   * The file ID used for training. You can retrieve the training data with the
   * [Files API](/docs/api-reference/files/retrieve-contents).
   */
  training_file: fc.string(),
  /**
   * The file ID used for validation. You can retrieve the validation results with the
   * [Files API](/docs/api-reference/files/retrieve-contents).
   */
  validation_file: fc.option(fc.string()),
  /**
   * The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the
   * [Files API](/docs/api-reference/files/retrieve-contents).
   */
  result_files: fc.array(fc.string()),
  /**
   * The total number of billable tokens processed by this fine tuning job. The value will be null
   * if the fine-tuning job is still running.
   */
  trained_tokens: fc.option(fc.maxSafeInteger()),
  /**
   * For fine-tuning jobs that have `failed`, this will contain more information on the cause of the
   * failure.
   */
  error: fc.option(fc.record(
    {
      /** A human-readable error message. */
      message: fc.string(),
      /** A machine-readable error code. */
      code: fc.string(),
      /**
       * The parameter that was invalid, usually `training_file` or `validation_file`. This field
       * will be null if the failure was not parameter-specific.
       */
      param: fc.option(fc.string()),
    },
    { withDeletedKeys: true },
  )),
});

/** Describes an OpenAI model offering that can be used with the API. */
const Model = fc.record({
  /** The model identifier, which can be referenced in the API endpoints. */
  id: fc.string(),
  /** The object type, which is always "model". */
  object: fc.constant("model"),
  /** The Unix timestamp (in seconds) when the model was created. */
  created: fc.string(),
  /** The organization that owns the model. */
  owned_by: fc.string(),
});

/** Represents an embedding vector returned by embedding endpoint. */
const Embedding = fc.record({
  /** The index of the embedding in the list of embeddings. */
  index: fc.maxSafeInteger(),
  /** The object type, which is always "embedding". */
  object: fc.constant("embedding"),
  /**
   * The embedding vector, which is a list of floats. The length of vector depends on the model as\
   * listed in the [embedding guide](/docs/guides/embeddings).
   */
  embedding: fc.array(fc.double()),
});

const TokenArray = fc.array(fc.maxSafeInteger(), { minLength: 1 });

const TokenArrayArray = fc.array(TokenArray, { minLength: 1 });

/** Usage statistics for the completion request. */
const CompletionUsage = fc.record({
  /** Number of tokens in the prompt. */
  prompt_tokens: fc.maxSafeInteger(),
  /** Number of tokens in the generated completion */
  completion_tokens: fc.maxSafeInteger(),
  /** Total number of tokens used in the request (prompt + completion). */
  total_tokens: fc.maxSafeInteger(),
});

const EditN = fc.nat({ max: 20 });

const Temperature = fc.float({
  min: 0,
  max: 2,
});

const TopP = fc.float({
  min: 0,
  max: 1,
});

const N = fc.integer({
  min: 1,
  max: 128,
});

const MaxTokens = fc.maxSafeNat();

const StopSequences = fc.array(fc.string(), {
  minLength: 1,
  maxLength: 4,
});

const Stop = fc.option(fc.oneof(
  fc.string(),
  StopSequences,
));

const Penalty = fc.float({
  min: -2,
  max: 2,
});

const Prompt = fc.option(fc.oneof(
  fc.string(),
  fc.array(fc.string()),
  TokenArray,
  TokenArrayArray,
));

const ChatCompletionResponseMessage = fc.record(
  {
    /** The role of the author of this message. */
    role: fc.constantFrom("system", "user", "assistant", "function"),
    /** The contents of the message. */
    content: fc.option(fc.string()),
    /** The name and arguments of a function that should be called, as generated by the model. */
    function_call: fc.record({
      /** The name of the function to call. */
      name: fc.string(),
      /**
       * The arguments to call the function with, as generated by the model in JSON format. Note that
       * the model does not always generate valid JSON, and may hallucinate parameters not defined by
       * your function schema. Validate the arguments in your code before calling your function.
       */
      arguments: fc.string(),
    }),
  },
  {
    requiredKeys: ["role", "content"],
  },
);

const ChatCompletionFunctionCallOption = fc.record({
  /** The name of the function to call. */
  name: fc.string(),
});

const ChatCompletionFunctionParameters = fc.dictionary(fc.string(), fc.anything());

const ChatCompletionFunctions = fc.record(
  {
    /**
     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
     * dashes, with a maximum length of 64.
     */
    name: fc.string(),
    /**
     * A description of what the function does, used by the model to choose when and how to call the
     * function.
     */
    description: fc.string(),
    /**
     * The parameters the functions accepts, described as a JSON Schema object. See the
     * [guide](/docs/guides/gpt/function-calling) for examples, and the
     * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation
     * about the format.\n\nTo describe a function that accepts no parameters, provide the value
     * `{\"type\": \"object\", \"properties\": {}}`.
     */
    parameters: ChatCompletionFunctionParameters,
  },
  {
    requiredKeys: ["name", "parameters"],
  },
);

const ChatCompletionRequestMessage = fc.record(
  {
    /** The role of the messages author. One of `system`, `user`, `assistant`, or `function`. */
    role: fc.constantFrom("system", "user", "assistant", "function"),
    /**
     * The contents of the message. `content` is required for all messages, and may be null for
     * assistant messages with function calls.
     */
    content: fc.option(fc.string()),
    /**
     * The name of the author of this message. `name` is required if role is `function`, and it
     * should be the name of the function whose response is in the `content`. May contain a-z,
     * A-Z, 0-9, and underscores, with a maximum length of 64 characters.
     */
    name: fc.string(),
    /** The name and arguments of a function that should be called, as generated by the model. */
    function_call: fc.record({
      /** The name of the function to call. */
      name: fc.string(),
      /**
       * The arguments to call the function with, as generated by the model in JSON format. Note that
       * the model does not always generate valid JSON, and may hallucinate parameters not defined by
       * your function schema. Validate the arguments in your code before calling your function.
       */
      arguments: fc.string(),
    }),
  },
  {
    requiredKeys: ["role", "content"],
  },
);

const Error = fc.record({
  type: fc.string(),
  message: fc.string(),
  param: fc.option(fc.string()),
  code: fc.option(fc.string()),
});

/** The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details. */
export const OpenAI = {
  Audio: {},

  Chat: {},

  FineTuning: {},

  Temperature,

  TopP,

  N,

  MaxTokens,

  Penalty,

  User,

  EditN,

  NEpochs,

  SuffixString,

  ImagesN,

  Stop,

  Prompt,

  CreateTranscriptionRequest: fc.record(
    {
      /**
       * The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4,
       * mpeg, mpga, m4a, ogg, wav, or webm.
       */
      file: fc.uint8Array(),
      /** ID of the model to use. Only `whisper-1` is currently available. */
      model: fc.oneof(
        fc.string(),
        fc.constant("whisper-1"),
      ),
      /**
       * An optional text to guide the model's style or continue a previous audio segment. The
       * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
       */
      prompt: fc.string(),
      /**
       * The format of the transcript output, in one of these options: json, text, srt, verbose_json, or
       * vtt.
       */
      response_format: fc.constantFrom("json", "text", "srt", "verbose_json", "vtt"),
      /**
       * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more
       * random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,
       * the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to
       * automatically increase the temperature until certain thresholds are hit.
       */
      temperature: fc.oneof(
        fc.float({
          min: 0,
          max: 1,
        }),
        fc.constant(0),
      ),
      /**
       * The language of the input audio. Supplying the input language in
       * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy
       * and latency.
       */
      language: fc.string(),
    },
    {
      requiredKeys: ["file", "model"],
    },
  ),

  CreateTranscriptionResponse: fc.record({
    text: fc.string(),
  }),

  ErrorResponse: fc.record({
    error: Error,
  }),

  Error,

  CreateTranslationRequest: fc.record(
    {
      /**
       * The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4,
       * mpeg, mpga, m4a, ogg, wav, or webm.
       */
      file: fc.uint8Array(),
      /** ID of the model to use. Only `whisper-1` is currently available. */
      model: fc.oneof(
        fc.string(),
        fc.constant("whisper-1"),
      ),
      /**
       * An optional text to guide the model's style or continue a previous audio segment. The
       * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
       */
      prompt: fc.string(),
      /**
       * The format of the transcript output, in one of these options: json, text, srt, verbose_json, or
       * vtt.
       */
      response_format: fc.constantFrom("json", "text", "srt", "verbose_json", "vtt"),
      /**
       * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more
       * random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,
       * the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to
       * automatically increase the temperature until certain thresholds are hit.
       */
      temperature: fc.oneof(
        fc.float({
          min: 0,
          max: 1,
        }),
        fc.constant(0),
      ),
    },
    {
      requiredKeys: ["file", "model"],
    },
  ),

  CreateTranslationResponse: fc.record({
    text: fc.string(),
  }),

  CreateChatCompletionRequest: fc.record(
    {
      /**
       * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
       * more random, while lower values like 0.2 will make it more focused and deterministic.
       *
       * We generally recommend altering this or `top_p` but not both.
       */
      temperature: fc.option(fc.oneof(
        Temperature,
        fc.constant(1),
      )),
      /**
       * An alternative to sampling with temperature, called nucleus sampling, where the model considers
       * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
       * the top 10% probability mass are considered.
       *
       * We generally recommend altering this or `temperature` but not both.
       */
      top_p: fc.option(fc.oneof(
        TopP,
        fc.constant(1),
      )),
      /**
       * How many completions to generate for each prompt.
       * **Note:** Because this parameter generates many completions, it can quickly consume your token
       * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
       */
      n: fc.option(fc.oneof(
        N,
        fc.constant(1),
      )),
      /**
       * The maximum number of [tokens](/tokenizer) to generate in the completion.
       *
       * The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
       * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
       * for counting tokens.
       */
      max_tokens: fc.option(fc.oneof(
        MaxTokens,
        fc.constant(16),
      )),
      /** Up to 4 sequences where the API will stop generating further tokens. */
      stop: fc.option(Stop),
      /**
       * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
       * in the text so far, increasing the model's likelihood to talk about new topics.
       *
       * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
       */
      presence_penalty: fc.option(Penalty),
      /**
       * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
       * frequency in the text so far, decreasing the model's likelihood to repeat the same line
       * verbatim.
       *
       * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
       */
      frequency_penalty: fc.option(Penalty),
      /**
       * Modify the likelihood of specified tokens appearing in the completion.
       * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
       * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
       * generated by the model prior to sampling. The exact effect will vary per model, but values
       * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
       * should result in a ban or exclusive selection of the relevant token.
       */
      logit_bias: fc.option(fc.dictionary(fc.string(), fc.maxSafeInteger())),
      /**
       * A unique identifier representing your end-user, which can help OpenAI to monitor and detect
       * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
       */
      user: User,
      /**
       * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
       * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
       * as they become available, with the stream terminated by a `data: [DONE]` message.
       * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
       */
      stream: fc.option(fc.boolean()),
      /**
       * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility)
       * table for details on which models work with the Chat API.
       */
      model: fc.oneof(
        fc.string(),
        fc.constantFrom("gpt4", "gpt-4-0314", "gpt-4-0613", "gpt-4-32k", "gpt-4-32k-0314", "gpt-4-32k-0613", "gpt-3.5-turbo", "gpt-3.5-turbo-16k", "gpt-3.5-turbo-0301", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613"),
      ),
      /**
       * A list of messages comprising the conversation so far.
       * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
       */
      messages: fc.array(ChatCompletionRequestMessage, { minLength: 1 }),
      /** A list of functions the model may generate JSON inputs for. */
      functions: fc.array(ChatCompletionFunctions, {
        minLength: 1,
        maxLength: 128,
      }),
      /**
       * Controls how the model responds to function calls. `none` means the model does not call a
       * function, and responds to the end-user. `auto` means the model can pick between an end-user or
       * calling a function.  Specifying a particular function via `{\"name":\ \"my_function\"}` forces the
       * model to call that function. `none` is the default when no functions are present. `auto` is the
       * default if functions are present.
       */
      function_call: fc.oneof(
        ChatCompletionFunctionCallOption,
        fc.constantFrom("none", "auto"),
      ),
    },
    {
      requiredKeys: ["model", "messages"],
    },
  ),

  ChatCompletionRequestMessage,

  ChatCompletionFunctions,

  ChatCompletionFunctionParameters,

  ChatCompletionFunctionCallOption,

  StopSequences,

  /** Represents a chat completion response returned by model, based on the provided input. */
  CreateChatCompletionResponse: fc.record(
    {
      /** A unique identifier for the chat completion. */
      id: fc.string(),
      /** The object type, which is always `chat.completion`. */
      object: fc.string(),
      /** The Unix timestamp (in seconds) of when the chat completion was created. */
      created: fc.string(),
      /** The model used for the chat completion. */
      model: fc.string(),
      /** A list of chat completion choices. Can be more than one if `n` is greater than 1. */
      choices: fc.array(fc.record({
        /** The index of the choice in the list of choices. */
        index: fc.maxSafeInteger(),
        message: ChatCompletionResponseMessage,
        /**
         * The reason the model stopped generating tokens. This will be `stop` if the model hit a
         * natural stop point or a provided stop sequence, `length` if the maximum number of tokens
         * specified in the request was reached, `content_filter` if the content was omitted due to
         * a flag from our content filters, or `function_call` if the model called a function.
         */
        finish_reason: fc.constantFrom("stop", "length", "function_call", "content_filter"),
      })),
      usage: CompletionUsage,
    },
    {
      requiredKeys: ["id", "object", "created", "model", "choices"],
    },
  ),

  ChatCompletionResponseMessage,

  /** Usage statistics for the completion request. */
  CompletionUsage,

  CreateCompletionRequest: fc.record(
    {
      /**
       * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
       * more random, while lower values like 0.2 will make it more focused and deterministic.
       *
       * We generally recommend altering this or `top_p` but not both.
       */
      temperature: fc.option(fc.oneof(
        Temperature,
        fc.constant(1),
      )),
      /**
       * An alternative to sampling with temperature, called nucleus sampling, where the model considers
       * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
       * the top 10% probability mass are considered.
       *
       * We generally recommend altering this or `temperature` but not both.
       */
      top_p: fc.option(fc.oneof(
        TopP,
        fc.constant(1),
      )),
      /**
       * How many completions to generate for each prompt.
       * **Note:** Because this parameter generates many completions, it can quickly consume your token
       * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
       */
      n: fc.option(fc.oneof(
        N,
        fc.constant(1),
      )),
      /**
       * The maximum number of [tokens](/tokenizer) to generate in the completion.
       *
       * The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
       * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
       * for counting tokens.
       */
      max_tokens: fc.option(fc.oneof(
        MaxTokens,
        fc.constant(16),
      )),
      /** Up to 4 sequences where the API will stop generating further tokens. */
      stop: fc.option(Stop),
      /**
       * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
       * in the text so far, increasing the model's likelihood to talk about new topics.
       *
       * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
       */
      presence_penalty: fc.option(Penalty),
      /**
       * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
       * frequency in the text so far, decreasing the model's likelihood to repeat the same line
       * verbatim.
       *
       * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
       */
      frequency_penalty: fc.option(Penalty),
      /**
       * Modify the likelihood of specified tokens appearing in the completion.
       * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
       * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
       * generated by the model prior to sampling. The exact effect will vary per model, but values
       * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
       * should result in a ban or exclusive selection of the relevant token.
       */
      logit_bias: fc.option(fc.dictionary(fc.string(), fc.maxSafeInteger())),
      /**
       * A unique identifier representing your end-user, which can help OpenAI to monitor and detect
       * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
       */
      user: User,
      /**
       * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
       * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
       * as they become available, with the stream terminated by a `data: [DONE]` message.
       * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
       */
      stream: fc.option(fc.boolean()),
      /**
       * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
       * see all of your available models, or see our [Model overview](/docs/models/overview) for
       * descriptions of them.
       */
      model: fc.oneof(
        fc.string(),
        fc.constantFrom("babbage-002", "davinci-002", "text-davinci-003", "text-davinci-002", "text-davinci-001", "code-davinci-002", "text-curie-001", "text-babbage-001", "text-ada-001"),
      ),
      /**
       * The prompt(s) to generate completions for, encoded as a string, array of strings, array of
       * tokens, or array of token arrays.
       *
       * Note that <|endoftext|> is the document separator that the model sees during training, so if a
       * prompt is not specified the model will generate as if from the beginning of a new document.
       */
      prompt: fc.oneof(
        Prompt,
        fc.constant("<|endoftext|>"),
      ),
      /** The suffix that comes after a completion of inserted text. */
      suffix: fc.option(fc.string()),
      /**
       * Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens.
       * For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The
       * API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1`
       * elements in the response.
       *
       * The maximum value for `logprobs` is 5.
       */
      logprobs: fc.option(fc.maxSafeInteger()),
      /** Echo back the prompt in addition to the completion */
      echo: fc.option(fc.boolean()),
      /**
       * Generates `best_of` completions server-side and returns the "best" (the one with the highest
       * log probability per token). Results cannot be streamed.
       *
       * When used with `n`, `best_of` controls the number of candidate completions and `n` specifies
       * how many to return â€“ `best_of` must be greater than `n`.
       *
       * **Note:** Because this parameter generates many completions, it can quickly consume your token
       * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
       */
      best_of: fc.option(fc.oneof(
        fc.maxSafeInteger(),
        fc.constant(1),
      )),
    },
    {
      requiredKeys: ["model"],
    },
  ),

  TokenArray,

  TokenArrayArray,

  /**
   * Represents a completion response from the API. Note: both the streamed and non-streamed response
   * objects share the same shape (unlike the chat endpoint).
   */
  CreateCompletionResponse: fc.record(
    {
      /** A unique identifier for the completion. */
      id: fc.string(),
      /** The object type, which is always `text_completion`. */
      object: fc.string(),
      /** The Unix timestamp (in seconds) of when the completion was created. */
      created: fc.string(),
      /** The model used for the completion. */
      model: fc.string(),
      /** The list of completion choices the model generated for the input. */
      choices: fc.array(fc.record({
        index: fc.maxSafeInteger(),
        text: fc.string(),
        logprobs: fc.option(fc.record({
          tokens: fc.array(fc.string()),
          token_logprobs: fc.array(fc.double()),
          top_logprobs: fc.array(fc.dictionary(fc.string(), fc.maxSafeInteger())),
          text_offset: fc.array(fc.maxSafeInteger()),
        })),
        /**
         * The reason the model stopped generating tokens. This will be `stop` if the model hit a
         * natural stop point or a provided stop sequence, or `content_filter` if content was omitted
         * due to a flag from our content filters, `length` if the maximum number of tokens specified
         * in the request was reached, or `content_filter` if content was omitted due to a flag from our
         * content filters.
         */
        finish_reason: fc.constantFrom("stop", "length", "content_filter"),
      })),
      usage: CompletionUsage,
    },
    {
      requiredKeys: ["id", "object", "created", "model", "choices"],
    },
  ),

  CreateEditRequest: fc.record(
    {
      /**
       * ID of the model to use. You can use the `text-davinci-edit-001` or `code-davinci-edit-001`
       * model with this endpoint.
       */
      model: fc.oneof(
        fc.string(),
        fc.constantFrom("text-davinci-edit-001", "code-davinci-edit-001"),
      ),
      /** The input text to use as a starting point for the edit. */
      input: fc.option(fc.oneof(
        fc.string(),
        fc.constant(""),
      )),
      /** The instruction that tells the model how to edit the prompt. */
      instruction: fc.string(),
      /** How many edits to generate for the input and instruction. */
      n: fc.option(fc.oneof(
        EditN,
        fc.constant(1),
      )),
      /**
       * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
       * more random, while lower values like 0.2 will make it more focused and deterministic.
       *
       * We generally recommend altering this or `top_p` but not both.
       */
      temperature: fc.option(fc.oneof(
        Temperature,
        fc.constant(1),
      )),
      /**
       * An alternative to sampling with temperature, called nucleus sampling, where the model considers
       * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
       * the top 10% probability mass are considered.
       *
       * We generally recommend altering this or `temperature` but not both.
       */
      top_p: fc.option(fc.oneof(
        TopP,
        fc.constant(1),
      )),
    },
    {
      requiredKeys: ["model", "instruction"],
    },
  ),

  CreateEditResponse: fc.record({
    /** The object type, which is always `edit`. */
    object: fc.constant("edit"),
    /** The Unix timestamp (in seconds) of when the edit was created. */
    created: fc.string(),
    /** description: A list of edit choices. Can be more than one if `n` is greater than 1. */
    choices: fc.array(fc.record({
      /** The edited result. */
      text: fc.string(),
      /** The index of the choice in the list of choices. */
      index: fc.maxSafeInteger(),
      /**
       * The reason the model stopped generating tokens. This will be `stop` if the model hit a
       * natural stop point or a provided stop sequence, or `length` if the maximum number of tokens
       * specified in the request was reached.
       */
      finish_reason: fc.constantFrom("stop", "length"),
    })),
    usage: CompletionUsage,
  }),

  CreateEmbeddingRequest: fc.record(
    {
      /** ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them. */
      model: fc.oneof(
        fc.string(),
        fc.constant("text-embedding-ada-002"),
      ),
      /**
       * Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a
       * single request, pass an array of strings or array of token arrays. Each input must not exceed
       * the max input tokens for the model (8191 tokens for `text-embedding-ada-002`) and cannot be an empty string.
       * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
       * for counting tokens.
       */
      input: fc.oneof(
        fc.string(),
        fc.array(fc.string()),
        TokenArray,
        TokenArrayArray,
      ),
      user: User,
    },
    {
      requiredKeys: ["model", "input"],
    },
  ),

  CreateEmbeddingResponse: fc.record({
    /** The object type, which is always "embedding". */
    object: fc.constant("embedding"),
    /** The name of the model used to generate the embedding. */
    model: fc.string(),
    /** The list of embeddings generated by the model. */
    data: fc.array(Embedding),
    /** The usage information for the request. */
    usage: fc.record({
      /** The number of tokens used by the prompt. */
      prompt_tokens: fc.maxSafeInteger(),
      /** The total number of tokens used by the request. */
      total_tokens: fc.maxSafeInteger(),
    }),
  }),

  /** Represents an embedding vector returned by embedding endpoint. */
  Embedding,

  ListModelsResponse: fc.record({
    object: fc.string(),
    data: fc.array(Model),
  }),

  /** Describes an OpenAI model offering that can be used with the API. */
  Model,

  DeleteModelResponse: fc.record({
    id: fc.string(),
    object: fc.string(),
    deleted: fc.boolean(),
  }),

  ListFilesResponse: fc.record({
    object: fc.string(),
    data: fc.array(OpenAIFile),
  }),

  /** The `File` object represents a document that has been uploaded to OpenAI. */
  OpenAIFile,

  CreateFileRequest: fc.record({
    /**
     * Name of the [JSON Lines](https://jsonlines.readthedocs.io/en/latest/) file to be uploaded.
     *
     * If the `purpose` is set to "fine-tune", the file will be used for fine-tuning.
     */
    file: fc.uint8Array(),
    /**
     * The intended purpose of the uploaded documents. Use "fine-tune" for
     * [fine-tuning](/docs/api-reference/fine-tuning). This allows us to validate the format of the
     * uploaded file.
     */
    purpose: fc.string(),
  }),

  DeleteFileResponse: fc.record({
    id: fc.string(),
    object: fc.string(),
    deleted: fc.boolean(),
  }),

  CreateFineTuningJobRequest: fc.record(
    {
      /**
       * The ID of an uploaded file that contains training data.
       *
       * See [upload file](/docs/api-reference/files/upload) for how to upload a file.
       *
       * Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with
       * the purpose `fine-tune`.
       *
       * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
       */
      training_file: fc.string(),
      /**
       * The ID of an uploaded file that contains validation data.
       *
       * If you provide this file, the data is used to generate validation metrics periodically during
       * fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should
       * not be present in both train and validation files.
       *
       * Your dataset must be formatted as a JSONL file. You must upload your file with the purpose
       * `fine-tune`.
       *
       * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
       */
      validation_file: fc.option(fc.string()),
      /**
       * The name of the model to fine-tune. You can select one of the
       * [supported models](/docs/guides/fine-tuning/what-models-can-be-fine-tuned).
       */
      model: fc.oneof(
        fc.string(),
        fc.constantFrom("babbage-002", "davinci-002", "gpt-3.5-turbo"),
      ),
      /** The hyperparameters used for the fine-tuning job. */
      hyperparameters: fc.record(
        {
          /**
           * The number of epochs to train the model for. An epoch refers to one full cycle through the
           * training dataset.
           */
          n_epochs: fc.oneof(
            NEpochs,
            fc.constant("auto"),
          ),
        },
        { withDeletedKeys: true },
      ),
      /**
       * A string of up to 18 characters that will be added to your fine-tuned model name.
       *
       * For example, a `suffix` of "custom-model-name" would produce a model name like
       * `ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel`.
       */
      suffix: fc.option(SuffixString),
    },
    {
      requiredKeys: ["training_file", "model"],
    },
  ),

  FineTuningJob,

  ListPaginatedFineTuningJobsResponse: fc.record({
    object: fc.string(),
    data: fc.array(FineTuningJob),
    has_more: fc.boolean(),
  }),

  ListFineTuningJobEventsResponse: fc.record({
    object: fc.string(),
    data: fc.array(FineTuningJobEvent),
  }),

  FineTuningJobEvent,

  CreateFineTuneRequest: fc.record(
    {
      /**
       * The ID of an uploaded file that contains training data.
       *
       * See [upload file](/docs/api-reference/files/upload) for how to upload a file.
       *
       * Your dataset must be formatted as a JSONL file, where each training example is a JSON object
       * with the keys "prompt" and "completion". Additionally, you must upload your file with the
       * purpose `fine-tune`.
       *
       * See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/creating-training-data) for more
       * details.
       */
      training_file: fc.string(),
      /**
       * The ID of an uploaded file that contains validation data.
       *
       * If you provide this file, the data is used to generate validation metrics periodically during
       * fine-tuning. These metrics can be viewed in the
       * [fine-tuning results file](/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).
       * Your train and validation data should be mutually exclusive.
       *
       * Your dataset must be formatted as a JSONL file, where each validation example is a JSON object
       * with the keys "prompt" and "completion". Additionally, you must upload your file with the
       * purpose `fine-tune`.
       *
       * See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/creating-training-data) for more
       * details.
       */
      validation_file: fc.option(fc.string()),
      /**
       * The name of the base model to fine-tune. You can select one of "ada", "babbage", "curie",
       * "davinci", or a fine-tuned model created after 2022-04-21 and before 2023-08-22. To learn more
       * about these models, see the [Models](/docs/models) documentation.
       */
      model: fc.option(fc.oneof(
        fc.string(),
        fc.constantFrom("ada", "babbage", "curie", "davinci"),
      )),
      /**
       * The number of epochs to train the model for. An epoch refers to one full cycle through the
       * training dataset.
       */
      n_epochs: fc.option(fc.oneof(
        fc.maxSafeInteger(),
        fc.constant(4),
      )),
      /**
       * The batch size to use for training. The batch size is the number of training examples used to
       * train a single forward and backward pass.
       *
       * By default, the batch size will be dynamically configured to be ~0.2% of the number of examples
       * in the training set, capped at 256 - in general, we've found that larger batch sizes tend to
       * work better for larger datasets.
       */
      batch_size: fc.option(fc.maxSafeInteger()),
      /**
       * The learning rate multiplier to use for training. The fine-tuning learning rate is the original
       * learning rate used for pretraining multiplied by this value.
       *
       * By default, the learning rate multiplier is the 0.05, 0.1, or 0.2 depending on final
       * `batch_size` (larger learning rates tend to perform better with larger batch sizes). We
       * recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best
       * results.
       */
      learning_rate_multiplier: fc.option(fc.double()),
      /**
       * The weight to use for loss on the prompt tokens. This controls how much the model tries to
       * learn to generate the prompt (as compared to the completion which always has a weight of 1.0),
       * and can add a stabilizing effect to training when completions are short.
       *
       * If prompts are extremely long (relative to completions), it may make sense to reduce this
       * weight so as to avoid over-prioritizing learning the prompt.
       */
      prompt_loss_rate: fc.option(fc.oneof(
        fc.double(),
        fc.constant(0.01),
      )),
      /**
       * If set, we calculate classification-specific metrics such as accuracy and F-1 score using the
       * validation set at the end of every epoch. These metrics can be viewed in the
       * [results file](/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).
       *
       * In order to compute classification metrics, you must provide a `validation_file`. Additionally,
       * you must specify `classification_n_classes` for multiclass classification or
       * `classification_positive_class` for binary classification.
       */
      compute_classification_metrics: fc.option(fc.boolean()),
      /**
       * The number of classes in a classification task.
       *
       * This parameter is required for multiclass classification.
       */
      classification_n_classes: fc.option(fc.maxSafeInteger()),
      /**
       * The positive class in binary classification.
       *
       * This parameter is needed to generate precision, recall, and F1 metrics when doing binary
       * classification.
       */
      classification_positive_class: fc.option(fc.string()),
      /**
       * If this is provided, we calculate F-beta scores at the specified beta values. The F-beta score
       * is a generalization of F-1 score. This is only used for binary classification.
       *
       * With a beta of 1 (i.e. the F-1 score), precision and recall are given the same weight. A larger
       * beta score puts more weight on recall and less on precision. A smaller beta score puts more
       * weight on precision and less on recall.
       */
      classification_betas: fc.option(fc.array(fc.double())),
      /**
       * A string of up to 18 characters that will be added to your fine-tuned model name.
       *
       * For example, a `suffix` of "custom-model-name" would produce a model name like
       * `ada:ft-your-org:custom-model-name-2022-02-15-04-21-04`.
       */
      suffix: fc.option(SuffixString),
    },
    {
      requiredKeys: ["training_file"],
    },
  ),

  /** The `FineTune` object represents a legacy fine-tune job that has been created through the API. */
  FineTune,

  FineTuneEvent,

  ListFineTunesResponse: fc.record({
    object: fc.string(),
    data: fc.array(FineTune),
  }),

  ListFineTuneEventsResponse: fc.record({
    object: fc.string(),
    data: fc.array(FineTuneEvent),
  }),

  FineTuningEvent: fc.record(
    {
      object: fc.string(),
      created_at: fc.string(),
      level: fc.string(),
      message: fc.string(),
      data: fc.option(fc.dictionary(fc.string(), fc.anything())),
      type: fc.constantFrom("message", "metrics"),
    },
    {
      requiredKeys: ["object", "created_at", "level", "message"],
    },
  ),

  CreateImageRequest: fc.record(
    {
      /** The number of images to generate. Must be between 1 and 10. */
      n: fc.option(fc.oneof(
        ImagesN,
        fc.constant(1),
      )),
      /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */
      size: fc.option(fc.constantFrom("256x256", "512x512", "1024x1024")),
      /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */
      response_format: fc.option(fc.constantFrom("url", "b64_json")),
      user: User,
      /** A text description of the desired image(s). The maximum length is 1000 characters. */
      prompt: fc.string(),
    },
    {
      requiredKeys: ["prompt"],
    },
  ),

  ImagesResponse: fc.record({
    created: fc.string(),
    data: fc.array(Image),
  }),

  /** Represents the url or the content of an image generated by the OpenAI API. */
  Image,

  CreateImageEditRequest: fc.record(
    {
      /** The number of images to generate. Must be between 1 and 10. */
      n: fc.option(fc.oneof(
        ImagesN,
        fc.constant(1),
      )),
      /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */
      size: fc.option(fc.constantFrom("256x256", "512x512", "1024x1024")),
      /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */
      response_format: fc.option(fc.constantFrom("url", "b64_json")),
      user: User,
      /** A text description of the desired image(s). The maximum length is 1000 characters. */
      prompt: fc.string(),
      /**
       * The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not
       * provided, image must have transparency, which will be used as the mask.
       */
      image: fc.uint8Array(),
      /**
       * An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where
       * `image` should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions
       * as `image`.
       */
      mask: fc.uint8Array(),
    },
    {
      requiredKeys: ["prompt", "image"],
    },
  ),

  CreateImageVariationRequest: fc.record(
    {
      /** The number of images to generate. Must be between 1 and 10. */
      n: fc.option(fc.oneof(
        ImagesN,
        fc.constant(1),
      )),
      /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */
      size: fc.option(fc.constantFrom("256x256", "512x512", "1024x1024")),
      /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */
      response_format: fc.option(fc.constantFrom("url", "b64_json")),
      user: User,
      /**
       * The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB,
       * and square.
       */
      image: fc.uint8Array(),
    },
    {
      requiredKeys: ["image"],
    },
  ),

  CreateModerationRequest: fc.record(
    {
      /** The input text to classify */
      input: fc.oneof(
        fc.string(),
        fc.array(fc.string()),
      ),
      /**
       * Two content moderations models are available: `text-moderation-stable` and
       * `text-moderation-latest`. The default is `text-moderation-latest` which will be automatically
       * upgraded over time. This ensures you are always using our most accurate model. If you use
       * `text-moderation-stable`, we will provide advanced notice before updating the model. Accuracy
       * of `text-moderation-stable` may be slightly lower than for `text-moderation-latest`.
       */
      model: fc.oneof(
        fc.string(),
        fc.constantFrom("text-moderation-latest", "text-moderation-stable"),
      ),
    },
    {
      requiredKeys: ["input"],
    },
  ),

  CreateModerationResponse: fc.record({
    /** The unique identifier for the moderation request. */
    id: fc.string(),
    /** The model used to generate the moderation results. */
    model: fc.string(),
    /** A list of moderation objects. */
    results: fc.array(fc.record({
      /** Whether the content violates [OpenAI's usage policies](/policies/usage-policies). */
      flagged: fc.boolean(),
      /** A list of the categories, and whether they are flagged or not. */
      categories: fc.record({
        /**
         * Content that expresses, incites, or promotes hate based on race, gender, ethnicity,
         * religion, nationality, sexual orientation, disability status, or caste. Hateful content
         * aimed at non-protected groups (e.g., chess players) is harrassment.
         */
        hate: fc.boolean(),
        /**
         * Hateful content that also includes violence or serious harm towards the targeted group
         * based on race, gender, ethnicity, religion, nationality, sexual orientation, disability
         * status, or caste.
         */
        "hate/threatening": fc.boolean(),
        /** Content that expresses, incites, or promotes harassing language towards any target. */
        harassment: fc.boolean(),
        /** Harassment content that also includes violence or serious harm towards any target. */
        "harassment/threatening": fc.boolean(),
        /**
         * Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting,
         * and eating disorders.
         */
        "self-harm": fc.boolean(),
        /**
         * Content where the speaker expresses that they are engaging or intend to engage in acts of
         * self-harm, such as suicide, cutting, and eating disorders.
         */
        "self-harm/intent": fc.boolean(),
        /**
         * Content that encourages performing acts of self-harm, such as suicide, cutting, and eating
         * disorders, or that gives instructions or advice on how to commit such acts.
         */
        "self-harm/instructive": fc.boolean(),
        /**
         * Content meant to arouse sexual excitement, such as the description of sexual activity, or
         * that promotes sexual services (excluding sex education and wellness).
         */
        sexual: fc.boolean(),
        /** Sexual content that includes an individual who is under 18 years old. */
        "sexual/minors": fc.boolean(),
        /** Content that depicts death, violence, or physical injury. */
        violence: fc.boolean(),
        /** Content that depicts death, violence, or physical injury in graphic detail. */
        "violence/graphic": fc.boolean(),
      }),
      /** A list of the categories along with their scores as predicted by model. */
      category_scores: fc.record({
        /** The score for the category 'hate'. */
        hate: fc.double(),
        /** The score for the category 'hate/threatening'. */
        "hate/threatening": fc.double(),
        /** The score for the category 'harassment'. */
        harassment: fc.double(),
        /** The score for the category 'harassment/threatening'. */
        "harassment/threatening": fc.double(),
        /** The score for the category 'self-harm'. */
        "self-harm": fc.double(),
        /** The score for the category 'self-harm/intent'. */
        "self-harm/intent": fc.double(),
        /** The score for the category 'self-harm/instructive'. */
        "self-harm/instructive": fc.double(),
        /** The score for the category 'sexual'. */
        sexual: fc.double(),
        /** The score for the category 'sexual/minors'. */
        "sexual/minors": fc.double(),
        /** The score for the category 'violence'. */
        violence: fc.double(),
        /** The score for the category 'violence/graphic'. */
        "violence/graphic": fc.double(),
      }),
    })),
  }),
};
